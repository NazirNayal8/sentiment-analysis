{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountHashtags(data):\n",
    "    \"\"\"\n",
    "    count words that start with # given a data string\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "        \n",
    "    count = len([s for s in data.split() if s.startswith('#')])\n",
    "    return count\n",
    "    \n",
    "\n",
    "def CountMentions(data):\n",
    "    \"\"\"\n",
    "    count words that start with @ given a data string\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    count = len([s for s in data.split() if s.startswith('@')])\n",
    "    return count\n",
    "\n",
    "def RemoveHashtags(data):\n",
    "    \"\"\"\n",
    "    Removes hashtags from a given string\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    hashtag_regex = '#[A-Za-z0-9]+'\n",
    "    \n",
    "    data = ' '.join(re.sub(hashtag_regex, ' ', data).split())\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "def RemoveMentions(data):\n",
    "    \"\"\"\n",
    "    Removes mentions from a given string\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    mention_regex = '@[A-Za-z0-9]+'\n",
    "    \n",
    "    data = ' '.join(re.sub(mention_regex, ' ', data).split())\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "def CountStopWords(data, stop_words):\n",
    "    \"\"\"\n",
    "    Given a text and a list of stop words, return count of step words in text\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    assert isinstance(stop_words, set)\n",
    "    \n",
    "    count = len([s for s in data.split() if s in stop_words])\n",
    "    return count\n",
    "\n",
    "def RemoveStopWords(data, stop_words):\n",
    "    \"\"\"\n",
    "    Given a test and a list of stop words, remove stop words from text\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    assert isinstance(stop_words, set)\n",
    "    \n",
    "    data = ' '.join([s for s in data.split() if s not in stop_words])\n",
    "    return data\n",
    "\n",
    "def GetWordCount(data):\n",
    "    \"\"\"\n",
    "    count number of words in a given text\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    count = len([s for s in data.split()])\n",
    "    return count\n",
    "\n",
    "def GetCharCount(data):\n",
    "    \"\"\"\n",
    "    count number of characters in a given text\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    return len(data)\n",
    "\n",
    "def GetAvgWordLength(data):\n",
    "    \"\"\"\n",
    "    Given a text, return average word length\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    words = data.split()\n",
    "    \n",
    "    Len = 0\n",
    "    for w in words:\n",
    "        Len += len(w)\n",
    "    \n",
    "    Ans = int(Len / len(words))\n",
    "    return Ans\n",
    "\n",
    "def GetNumericDigitsCount(data):\n",
    "    \"\"\"\n",
    "    Given a text, count number of numerical digits (not imbedded)\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    count = len([s for s in data.split() if s.isdigit()])\n",
    "    return count\n",
    "\n",
    "def CountContractions(data, contractions):\n",
    "    \"\"\"\n",
    "    Given a text and map of contractions, return count of contractions in text\n",
    "    \n",
    "    Make sure all characters are lowercase\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    assert isinstance(contractions, dict)\n",
    "    \n",
    "    count = len([s for s in data.split() if s in contractions])\n",
    "    return count\n",
    "\n",
    "def ExpandContractions(data, contractions):\n",
    "    \"\"\"\n",
    "    Given a text, and a map of contractions, replace contraction in the given text\n",
    "    \n",
    "    Make sure all character are lowercase\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    assert isinstance(contractions, dict)\n",
    "    \n",
    "    for key in contractions:\n",
    "        value = contractions[key]\n",
    "        data = data.replace(key, value)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def GetEmailCount(data):\n",
    "    \"\"\"\n",
    "    Given a text, count number of emails \n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    email_regex = '([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)'\n",
    "    \n",
    "    count = len(re.findall(email_regex, data))\n",
    "    return count\n",
    "\n",
    "def RemoveEmails(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove all emails from it\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    email_regex = email_regex = '([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)'\n",
    "        \n",
    "    data = re.sub(email_regex, '', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def ExtractEmails(data):\n",
    "    \"\"\"\n",
    "    Given a text, return all emails in this text\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    email_regex = '([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)'\n",
    "    \n",
    "    emails = re.findall(email_regex, data)\n",
    "    \n",
    "    return emails\n",
    "\n",
    "def GetUrlCount(data):\n",
    "    \"\"\"\n",
    "    Given a text, count Urls it contains\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    url_regex = '(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "    \n",
    "    count = len(re.findall(url_regex, data))\n",
    "    \n",
    "    return count\n",
    "\n",
    "def RemoveUrls(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove Urls it contains\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    url_regex = '(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "    \n",
    "    data = re.sub(url_regex, '', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def ExtractUrls(data):\n",
    "    \"\"\"\n",
    "    Given a text, return all Urls it contains\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    url_regex = '(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "    \n",
    "    urls = re.findall(url_regex, data)\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def RemoveSpecialChars(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove special (non-alphanumeric) characters\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    special_regex = '[^A-Z a-z 0-9-]+'\n",
    "    \n",
    "    data = re.sub(special_regex, '', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def RemoveMultipleSpaces(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove multiple whistepaces\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    data = ' '.join(data.split())\n",
    "    \n",
    "    return data\n",
    "\n",
    "def RemoveHTMLTags(data):\n",
    "    \"\"\"\n",
    "    Given a text remove any existing html tags\n",
    "    \n",
    "    Assuming BeautifulSoup is imported from bs4\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    data = BeautifulSoup(data, 'lxml').getText()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def RemoveAccentedChars(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove accented chars\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    data = unicodedata.normalize('NFKD', data).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return data\n",
    "\n",
    "def CorrectSpelling(data):\n",
    "    \"\"\"\n",
    "    Given a text, correct spelling of words using textblob\n",
    "    \n",
    "    Assuming TextBlob is imported from textblob\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    data = TextBlob(data).correct()\n",
    "    return data\n",
    "    \n",
    "\n",
    "def RemovePunctuations(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove its punctuation\n",
    "    \n",
    "    Assuming re is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    punc_regex = '[\\.\\,\\!\\?\\:\\;\\-\\=]'\n",
    "    \n",
    "    data = re.sub(punc_regex, '', data)\n",
    "    return data\n",
    "\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"btw\": \"by the way\",\n",
    "\"cuz\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"gonna\": \"going to\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how does\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\" u \": \" you \",\n",
    "\" ur \": \" your \",\n",
    "\" n \": \" and \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountSentiments(sentiments):\n",
    "    cnt0 = 0\n",
    "    cnt1 = 0\n",
    "    for sent in sentiments:\n",
    "        if sent == 1:\n",
    "            cnt1 += 1\n",
    "        else:\n",
    "            cnt0 += 1\n",
    "            \n",
    "    print(cnt0,cnt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>IDs</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment         IDs                          date      flag  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5          0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6          0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7          0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8          0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9          0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9          mimismo                          @twittera que me muera ?   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Columns = ['sentiment', 'IDs', 'date', 'flag', 'user', 'text']\n",
    "RawData = pd.read_csv('twitter_data.csv', encoding = 'ISO-8859-1', names = Columns)\n",
    "RawData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      "sentiment    1600000 non-null int64\n",
      "IDs          1600000 non-null int64\n",
      "date         1600000 non-null object\n",
      "flag         1600000 non-null object\n",
      "user         1600000 non-null object\n",
      "text         1600000 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "RawData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "RawData = RawData.drop(columns = ['IDs','date','flag','user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "N = 10000\n",
    "K = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@decryption Excellent  - Will you be in this W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>@hokeypokeyjones Get 100 followers a day using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Somehow, i think any David Bowie song would go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Has had a great weekend and is now back in wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@BarelySeeAtAll I love you!  I'm sure that Ren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          4  @decryption Excellent  - Will you be in this W...\n",
       "1          4  @hokeypokeyjones Get 100 followers a day using...\n",
       "2          4  Somehow, i think any David Bowie song would go...\n",
       "3          0  Has had a great weekend and is now back in wor...\n",
       "4          4  @BarelySeeAtAll I love you!  I'm sure that Ren..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle data\n",
    "RawData = RawData.sample(frac = 1).reset_index(drop = True)\n",
    "RawData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of the data points\n",
    "Data = RawData.iloc[0:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nazirnayal/.local/lib/python3.7/site-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@decryption Excellent  - Will you be in this W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@hokeypokeyjones Get 100 followers a day using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Somehow, i think any David Bowie song would go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Has had a great weekend and is now back in wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@BarelySeeAtAll I love you!  I'm sure that Ren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  @decryption Excellent  - Will you be in this W...\n",
       "1          1  @hokeypokeyjones Get 100 followers a day using...\n",
       "2          1  Somehow, i think any David Bowie song would go...\n",
       "3          0  Has had a great weekend and is now back in wor...\n",
       "4          1  @BarelySeeAtAll I love you!  I'm sure that Ren..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize sentiment\n",
    "Data.sentiment = Data.sentiment.apply(lambda x : (1 if x == 4 else 0))\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn to lowercase\n",
    "Data.text = Data.text.apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullText = ' '.join([s for s in Data.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hashtags: 283\n",
      "Number of Mentions: 4971\n",
      "Number of Stop Words: 56619\n",
      "Number of Emails: 4\n",
      "Number of Urls: 459\n",
      "Number of Words: 132364\n",
      "Number of chars: 754153\n",
      "Stop word percentage: 42.77522589223656\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(f\"Number of Hashtags: {CountHashtags(FullText)}\")\n",
    "print(f\"Number of Mentions: {CountMentions(FullText)}\")\n",
    "print(f\"Number of Stop Words: {CountStopWords(FullText,STOP_WORDS)}\")\n",
    "print(f\"Number of Emails: {GetEmailCount(FullText)}\")\n",
    "print(f\"Number of Urls: {GetUrlCount(FullText)}\")\n",
    "print(f\"Number of Words: {GetWordCount(FullText)}\")\n",
    "print(f\"Number of chars: {GetCharCount(FullText)}\")\n",
    "print(f\"Stop word percentage: {(CountStopWords(FullText,STOP_WORDS) / GetWordCount(FullText) * 100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Length: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Word Length: {GetAvgWordLength(FullText)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words for nltk:51198\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "print(f\"Stop Words for nltk:{CountStopWords(FullText,stop_words_nltk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.text = Data.text.apply(lambda x : RemoveHashtags(x))\n",
    "Data.text = Data.text.apply(lambda x : RemoveMentions(x))\n",
    "Data.text = Data.text.apply(lambda x : ExpandContractions(x, contractions))\n",
    "#Data.text = Data.text.apply(lambda x : RemoveStopWords(x, stop_words_nltk))\n",
    "Data.text = Data.text.apply(lambda x : RemoveEmails(x))\n",
    "Data.text = Data.text.apply(lambda x : RemoveUrls(x))\n",
    "Data.text = Data.text.apply(lambda x : RemoveSpecialChars(x))\n",
    "Data.text = Data.text.apply(lambda x : RemoveMultipleSpaces(x))\n",
    "Data.text = Data.text.apply(lambda x : RemovePunctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>excellent  will you be in this weekends edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>get 100 followers a day using wwwtweeterfollow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>somehow i think any david bowie song would go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>has had a great weekend and is now back in wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i love you i am sure that reno loves you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>quotstatistical mechanics  the theory of wiggl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>honestly i do think i might have a problem i p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>heheh hey whatever youre comfortable with  do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>im football illiterate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>good morning new followers time for my facesiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>i gotsta get into shape bits of me are startin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>going to work on graduation invitations today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>chicago cannot wait for the show tonight it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>we have angels watching out for us though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>good morning everyone today i have the whole d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>not anymore 8930pm mon and thurs are for tae k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>home alone again tonight  did get invited to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>yep lets hangout soon man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>missed you today hope you feel better lt3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>is making swiss roles first lesson at school a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                               text\n",
       "0           1    excellent  will you be in this weekends edition\n",
       "1           1  get 100 followers a day using wwwtweeterfollow...\n",
       "2           1  somehow i think any david bowie song would go ...\n",
       "3           0  has had a great weekend and is now back in wor...\n",
       "4           1           i love you i am sure that reno loves you\n",
       "5           1  quotstatistical mechanics  the theory of wiggl...\n",
       "6           0  honestly i do think i might have a problem i p...\n",
       "7           1  heheh hey whatever youre comfortable with  do ...\n",
       "8           0                             im football illiterate\n",
       "9           1  good morning new followers time for my facesiz...\n",
       "10          0  i gotsta get into shape bits of me are startin...\n",
       "11          0  going to work on graduation invitations today ...\n",
       "12          1  chicago cannot wait for the show tonight it is...\n",
       "13          0          we have angels watching out for us though\n",
       "14          1  good morning everyone today i have the whole d...\n",
       "15          1  not anymore 8930pm mon and thurs are for tae k...\n",
       "16          0  home alone again tonight  did get invited to a...\n",
       "17          1                          yep lets hangout soon man\n",
       "18          0          missed you today hope you feel better lt3\n",
       "19          0  is making swiss roles first lesson at school a..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorize(x):\n",
    "    x = x.lower()\n",
    "    document = nlp(x)\n",
    "    VecSum = np.zeros(shape = (1,K))\n",
    "    count = 0\n",
    "    for word in document:\n",
    "        if word.has_vector:\n",
    "            VecSum += word.vector.T\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return VecSum\n",
    "    return (VecSum / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 300)\n",
      "[[-5.08024981e-02  1.57572871e-01  2.56343745e-02 ... -1.50913117e-01\n",
      "   1.49098399e-02 -6.73260365e-05]\n",
      " [-9.93935556e-02  1.31370086e-01 -2.34986665e-01 ... -8.78941647e-02\n",
      "   9.42981668e-02  1.61424389e-01]\n",
      " [-4.33379243e-02  2.11437843e-01 -1.47872621e-01 ... -7.36254631e-02\n",
      "   5.44357681e-02  3.29791536e-01]\n",
      " ...\n",
      " [ 1.02721431e-02  2.90677138e-01 -1.40037101e-01 ... -1.08468000e-01\n",
      "  -8.72590026e-02  1.27690667e-01]\n",
      " [ 1.42092832e-02  1.97957242e-01 -2.84860964e-01 ... -5.25510386e-02\n",
      "  -1.52867605e-02  1.34494128e-01]\n",
      " [-2.19953119e-02  1.72259445e-01 -1.93147374e-01 ...  4.92433300e-03\n",
      "   5.32588513e-02  1.74703232e-01]]\n"
     ]
    }
   ],
   "source": [
    "DataMatrix = np.concatenate([Vectorize(text) for text in Data.text])\n",
    "print(DataMatrix.shape)\n",
    "print(DataMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store labels in numpy array\n",
    "Labels = Data['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DataTrain, DataTest, LabelTrain, LabelTest = train_test_split(DataMatrix, Labels, test_size = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_Model = SVC(kernel = 'linear')\n",
    "SVM_Model.fit(DataTrain, LabelTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n"
     ]
    }
   ],
   "source": [
    "print(SVM_Model.score(DataTest,LabelTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Dataa\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "H1 = 100\n",
    "H2 = 100\n",
    "OP = 1\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(K, H1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.prelu = nn.PReLU(1)\n",
    "        self.out = nn.Linear(H2, OP)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        a1 = self.fc1(input_)\n",
    "        h1 = self.relu1(a1)\n",
    "        dout = self.dout(h1)\n",
    "        a2 = self.fc2(dout)\n",
    "        h2 = self.prelu(a2)\n",
    "        a4 = self.out(h2)\n",
    "        y = self.out_act(a4)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attack(X,Y):\n",
    "    Net = NeuralNet()\n",
    "    Loss_Func = nn.BCELoss()\n",
    "    Optimizer = torch.optim.Adam(Net.parameters(), lr = LR, betas = (0.9, 0.999))\n",
    "    Torch_Dataset = Dataa.TensorDataset(X,Y)\n",
    "    Loader = Dataa.DataLoader(\n",
    "        dataset = Torch_Dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = 4,\n",
    "    )\n",
    "    for Epoch in range(EPOCH):\n",
    "        cnt = 0\n",
    "        s = 0\n",
    "        for step, (Batch_X,Batch_Y) in enumerate(Loader):\n",
    "            Optimizer.zero_grad()\n",
    "            B_X = Variable(Batch_X)\n",
    "            B_Y = Variable(Batch_Y)\n",
    "            Prediction = Net(B_X)\n",
    "            Loss = Loss_Func(Prediction.squeeze(),B_Y)\n",
    "            s = s + Loss.data.item()\n",
    "            cnt = cnt + 1\n",
    "            Loss.backward()\n",
    "            Optimizer.step()\n",
    "        print(s / cnt)\n",
    "    return Net\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5727913952465599\n",
      "0.5283152029869405\n",
      "0.5129052806407848\n",
      "0.503301259050978\n",
      "0.4889334204560476\n",
      "0.4790828196292228\n",
      "0.46563216536603075\n",
      "0.45349914089162296\n",
      "0.44014260843924596\n",
      "0.43403868419481506\n",
      "0.4207893597318771\n",
      "0.40985356292403335\n",
      "0.40633054130466273\n",
      "0.3899292562764587\n",
      "0.3788814920377224\n",
      "0.3701172758501472\n",
      "0.3654082367289151\n",
      "0.35705000500307016\n",
      "0.34057804833807\n",
      "0.33067188773911893\n",
      "0.32433915767052496\n",
      "0.31450869581589463\n",
      "0.3083854521102939\n",
      "0.30008496090452724\n",
      "0.2934312970238797\n",
      "0.2840999811095126\n",
      "0.27070400969567876\n",
      "0.2667044886249177\n",
      "0.25998048357507014\n",
      "0.2569349026901925\n",
      "0.24514316764812097\n",
      "0.2390230536751502\n",
      "0.23990552277958138\n",
      "0.230963454628033\n",
      "0.22715159031357748\n",
      "0.216809527803186\n",
      "0.21771765859625863\n",
      "0.20950479443508682\n",
      "0.19402031268898054\n",
      "0.19178557027369103\n",
      "0.20118439887115297\n",
      "0.1924921896123717\n",
      "0.18685515933002986\n",
      "0.17738680529626125\n",
      "0.18042389102650028\n",
      "0.18695806612835286\n",
      "0.1747049019004859\n",
      "0.1689807542249666\n",
      "0.1640797345139456\n",
      "0.1593276139064725\n",
      "0.1539257605566729\n",
      "0.16007211647187988\n",
      "0.16153021621128136\n",
      "0.15193267815237774\n",
      "0.14969715053308094\n",
      "0.14614965528883833\n",
      "0.16183830576400596\n",
      "0.13941130113081202\n",
      "0.1405793496651958\n",
      "0.1423071708913285\n",
      "0.14609038802061944\n",
      "0.13304833773893457\n",
      "0.13623878279212095\n",
      "0.1326214246441947\n",
      "0.12894727775153328\n",
      "0.1327555125896284\n",
      "0.13272346509785676\n",
      "0.12369118954867442\n",
      "0.12627142188238336\n",
      "0.12829993694611158\n",
      "0.12542100167496406\n",
      "0.13274798412222732\n",
      "0.11004173200485026\n",
      "0.12180333389117917\n",
      "0.11701359994815173\n",
      "0.10923911380938235\n",
      "0.1154119622421365\n",
      "0.11352659407210478\n",
      "0.1127408214560743\n",
      "0.10851787561418634\n",
      "0.1119512716689682\n",
      "0.1072665397507132\n",
      "0.10539740486630297\n",
      "0.1144424389539194\n",
      "0.10684266457571945\n",
      "0.10658209312522242\n",
      "0.10937066445067684\n",
      "0.09925651548641688\n",
      "0.08858553565880085\n",
      "0.10261391367802912\n",
      "0.09363530667379816\n",
      "0.11366526288944725\n",
      "0.10085217296996266\n",
      "0.0916069304085079\n",
      "0.09685570956042684\n",
      "0.1014007341318123\n",
      "0.09960049942981267\n",
      "0.08254872842077562\n",
      "0.09249462511106427\n",
      "0.10228806833987593\n"
     ]
    }
   ],
   "source": [
    "DataTensor = torch.from_numpy(DataTrain).float()\n",
    "LabelTensor = torch.from_numpy(LabelTrain).float()\n",
    "Network = Attack(DataTensor,LabelTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTestTensor = torch.from_numpy(DataTest).float()\n",
    "LabelTestTensor = torch.from_numpy(LabelTest).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid = Network(DataTestTensor).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = Valid.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mask = [(1 if val > 0.5 else 0) for val in Output]\n",
    "Accuracy = sum(Mask == LabelTest) / len(Mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694\n"
     ]
    }
   ],
   "source": [
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "2.2415590286254883\n",
      "0.2165079116821289\n",
      "[0. 5. 5. ... 5. 5. 5.]\n",
      "[    0     1     2 ... 99997 99998 99999]\n",
      "(100001,)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "append_test = np.zeros(1);\n",
    "concat_test = np.zeros(1);\n",
    "print(append_test.shape)\n",
    "\n",
    "append_start = time.time();\n",
    "\n",
    "for i in range(100000):\n",
    "    append_test = np.append(append_test, [5], axis = 0)\n",
    "\n",
    "append_time = time.time() - append_start\n",
    "\n",
    "concat_start = time.time()\n",
    "\n",
    "concat_test = np.concatenate([[el] for el in range(100000)])\n",
    "\n",
    "concat_time = time.time() - concat_start\n",
    "\n",
    "print(append_time)\n",
    "print(concat_time)\n",
    "\n",
    "print(append_test)\n",
    "print(concat_test)\n",
    "\n",
    "print(append_test.shape)\n",
    "print(concat_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
