{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountHashtags(data):\n",
    "    \"\"\"\n",
    "    count words that start with # given a data string\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "        \n",
    "    count = len([s for s in data.split() if s.startswith('#')])\n",
    "    return count\n",
    "    \n",
    "\n",
    "def CountMentions(data):\n",
    "    \"\"\"\n",
    "    count words that start with @ given a data string\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    count = len([s for s in data.split() if s.startswith('@')])\n",
    "    return count\n",
    "\n",
    "def RemoveHashtags(data):\n",
    "    \"\"\"\n",
    "    Removes hashtags from a given string\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    hashtag_regex = '#[A-Za-z0-9]+'\n",
    "    \n",
    "    data = ' '.join(re.sub(hashtag_regex, ' ', data).split())\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "def RemoveMentions(data):\n",
    "    \"\"\"\n",
    "    Removes mentions from a given string\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    mention_regex = '@[A-Za-z0-9]+'\n",
    "    \n",
    "    data = ' '.join(re.sub(mention_regex, ' ', data).split())\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "def CountStopWords(data, stop_words):\n",
    "    \"\"\"\n",
    "    Given a text and a list of stop words, return count of step words in text\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    assert isinstance(stop_words, set)\n",
    "    \n",
    "    count = len([s for s in data.split() if s in stop_words])\n",
    "    return count\n",
    "\n",
    "def RemoveStopWords(data, stop_words):\n",
    "    \"\"\"\n",
    "    Given a test and a list of stop words, remove stop words from text\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    assert isinstance(stop_words, set)\n",
    "    \n",
    "    data = ' '.join([s for s in data.split() if s not in stop_words])\n",
    "    return data\n",
    "\n",
    "def GetWordCount(data):\n",
    "    \"\"\"\n",
    "    count number of words in a given text\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    count = len([s for s in data.split()])\n",
    "    return count\n",
    "\n",
    "def GetCharCount(data):\n",
    "    \"\"\"\n",
    "    count number of characters in a given text\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    return len(data)\n",
    "\n",
    "def GetAvgWordLength(data):\n",
    "    \"\"\"\n",
    "    Given a text, return average word length\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    words = data.split()\n",
    "    \n",
    "    Len = 0\n",
    "    for w in words:\n",
    "        Len += len(w)\n",
    "    \n",
    "    Ans = int(Len / len(words))\n",
    "    return Ans\n",
    "\n",
    "def GetNumericDigitsCount(data):\n",
    "    \"\"\"\n",
    "    Given a text, count number of numerical digits (not imbedded)\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    count = len([s for s in data.split() if s.isdigit()])\n",
    "    return count\n",
    "\n",
    "def CountContractions(data, contractions):\n",
    "    \"\"\"\n",
    "    Given a text and map of contractions, return count of contractions in text\n",
    "    \n",
    "    Make sure all characters are lowercase\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    assert isinstance(contractions, dict)\n",
    "    \n",
    "    count = len([s for s in data.split() if s in contractions])\n",
    "    return count\n",
    "\n",
    "def ExpandContractions(data, contractions):\n",
    "    \"\"\"\n",
    "    Given a text, and a map of contractions, replace contraction in the given text\n",
    "    \n",
    "    Make sure all character are lowercase\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    assert isinstance(contractions, dict)\n",
    "    \n",
    "    for key in contractions:\n",
    "        value = contractions[key]\n",
    "        data = data.replace(key, value)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def GetEmailCount(data):\n",
    "    \"\"\"\n",
    "    Given a text, count number of emails \n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    email_regex = '([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)'\n",
    "    \n",
    "    count = len(re.findall(email_regex, data))\n",
    "    return count\n",
    "\n",
    "def RemoveEmails(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove all emails from it\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    email_regex = email_regex = '([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)'\n",
    "        \n",
    "    data = re.sub(email_regex, '', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def ExtractEmails(data):\n",
    "    \"\"\"\n",
    "    Given a text, return all emails in this text\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    email_regex = '([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)'\n",
    "    \n",
    "    emails = re.findall(email_regex, data)\n",
    "    \n",
    "    return emails\n",
    "\n",
    "def GetUrlCount(data):\n",
    "    \"\"\"\n",
    "    Given a text, count Urls it contains\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    url_regex = '(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "    \n",
    "    count = len(re.findall(url_regex, data))\n",
    "    \n",
    "    return count\n",
    "\n",
    "def RemoveUrls(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove Urls it contains\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    url_regex = '(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "    \n",
    "    data = re.sub(url_regex, '', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def ExtractUrls(data):\n",
    "    \"\"\"\n",
    "    Given a text, return all Urls it contains\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    url_regex = '(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "    \n",
    "    urls = re.findall(url_regex, data)\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def RemoveSpecialChars(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove special (non-alphanumeric) characters\n",
    "    \n",
    "    Assuming re library is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    special_regex = '[^A-Z a-z 0-9-]+'\n",
    "    \n",
    "    data = re.sub(special_regex, '', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def RemoveMultipleSpaces(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove multiple whistepaces\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    data = ' '.join(data.split())\n",
    "    \n",
    "    return data\n",
    "\n",
    "def RemoveHTMLTags(data):\n",
    "    \"\"\"\n",
    "    Given a text remove any existing html tags\n",
    "    \n",
    "    Assuming BeautifulSoup is imported from bs4\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    data = BeautifulSoup(data, 'lxml').getText()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def RemoveAccentedChars(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove accented chars\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    data = unicodedata.normalize('NFKD', data).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return data\n",
    "\n",
    "def CorrectSpelling(data):\n",
    "    \"\"\"\n",
    "    Given a text, correct spelling of words using textblob\n",
    "    \n",
    "    Assuming TextBlob is imported from textblob\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    data = TextBlob(data).correct()\n",
    "    return data\n",
    "    \n",
    "\n",
    "def RemovePunctuations(data):\n",
    "    \"\"\"\n",
    "    Given a text, remove its punctuation\n",
    "    \n",
    "    Assuming re is imported\n",
    "    \"\"\"\n",
    "    assert isinstance(data, str)\n",
    "    \n",
    "    punc_regex = '[\\.\\,\\!\\?\\:\\;\\-\\=]'\n",
    "    \n",
    "    data = re.sub(punc_regex, '', data)\n",
    "    return data\n",
    "\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"btw\": \"by the way\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"gonna\": \"going to\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how does\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\" u \": \" you \",\n",
    "\" ur \": \" your \",\n",
    "\" n \": \" and \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountSentiments(sentiments):\n",
    "    cnt0 = 0\n",
    "    cnt1 = 0\n",
    "    for sent in sentiments:\n",
    "        if sent == 1:\n",
    "            cnt1 += 1\n",
    "        else:\n",
    "            cnt0 += 1\n",
    "            \n",
    "    print(cnt0,cnt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>IDs</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment         IDs                          date      flag  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5          0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6          0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7          0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8          0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9          0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9          mimismo                          @twittera que me muera ?   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Columns = ['sentiment', 'IDs', 'date', 'flag', 'user', 'text']\n",
    "RawData = pd.read_csv('twitter_data.csv', encoding = 'ISO-8859-1', names = Columns)\n",
    "RawData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "RawData = RawData.drop(columns = ['IDs','date','flag','user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "N = 15000\n",
    "K = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Another case of the Monday blues. Exhausted fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>@JonathanRKnight rain + NKOTB = 5 sexy wet gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>just back from a gr8 walk under a cloudless bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Name a common filler used to simulate coconut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@MafiaShe i only has 12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  Another case of the Monday blues. Exhausted fr...\n",
       "1          4  @JonathanRKnight rain + NKOTB = 5 sexy wet gro...\n",
       "2          4  just back from a gr8 walk under a cloudless bl...\n",
       "3          4  Name a common filler used to simulate coconut ...\n",
       "4          0                           @MafiaShe i only has 12 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle data\n",
    "RawData = RawData.sample(frac = 1).reset_index(drop = True)\n",
    "RawData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of the data points\n",
    "Data = RawData.iloc[0:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nazirnayal/.local/lib/python3.7/site-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Another case of the Monday blues. Exhausted fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@JonathanRKnight rain + NKOTB = 5 sexy wet gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>just back from a gr8 walk under a cloudless bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Name a common filler used to simulate coconut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@MafiaShe i only has 12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  Another case of the Monday blues. Exhausted fr...\n",
       "1          1  @JonathanRKnight rain + NKOTB = 5 sexy wet gro...\n",
       "2          1  just back from a gr8 walk under a cloudless bl...\n",
       "3          1  Name a common filler used to simulate coconut ...\n",
       "4          0                           @MafiaShe i only has 12 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize sentiment\n",
    "Data.sentiment = Data.sentiment.apply(lambda x : (1 if x == 4 else 0))\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn to lowercase\n",
    "Data.text = Data.text.apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullText = ' '.join([s for s in Data.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hashtags: 424\n",
      "Number of Mentions: 7382\n",
      "Number of Stop Words: 83400\n",
      "Number of Emails: 6\n",
      "Number of Urls: 693\n",
      "Number of Words: 196494\n",
      "Number of chars: 1121195\n",
      "Stop word percentage: 42.444044092949405\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(f\"Number of Hashtags: {CountHashtags(FullText)}\")\n",
    "print(f\"Number of Mentions: {CountMentions(FullText)}\")\n",
    "print(f\"Number of Stop Words: {CountStopWords(FullText,STOP_WORDS)}\")\n",
    "print(f\"Number of Emails: {GetEmailCount(FullText)}\")\n",
    "print(f\"Number of Urls: {GetUrlCount(FullText)}\")\n",
    "print(f\"Number of Words: {GetWordCount(FullText)}\")\n",
    "print(f\"Number of chars: {GetCharCount(FullText)}\")\n",
    "print(f\"Stop word percentage: {(CountStopWords(FullText,STOP_WORDS) / GetWordCount(FullText) * 100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Length: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Word Length: {GetAvgWordLength(FullText)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words for nltk:75552\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "print(f\"Stop Words for nltk:{CountStopWords(FullText,stop_words_nltk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.text = Data.text.apply(lambda x : RemoveHashtags(x))\n",
    "Data.text = Data.text.apply(lambda x : RemoveMentions(x))\n",
    "Data.text = Data.text.apply(lambda x : ExpandContractions(x, contractions))\n",
    "Data.text = Data.text.apply(lambda x : RemoveStopWords(x, stop_words_nltk))\n",
    "Data.text = Data.text.apply(lambda x : RemoveEmails(x))\n",
    "Data.text = Data.text.apply(lambda x : RemoveUrls(x))\n",
    "Data.text = Data.text.apply(lambda x : RemoveSpecialChars(x))\n",
    "Data.text = Data.text.apply(lambda x : RemoveMultipleSpaces(x))\n",
    "Data.text = Data.text.apply(lambda x : RemovePunctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>another case monday blues exhausted thinking m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rain nkotb 5 sexy wet grown men wish concert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>back gr8 walk cloudless blue heaven lifes wond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>name common filler used simulate coconut milk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>im ya love rice green beans cheese fries crazi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0robertpatt now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>watching tele went dentist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>lolls thats pretty ace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>sinners would loved girly night in plan sumthi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>terrible headache need poison brain immediately</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>love penny sleeptime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>le sigh righti sure pretty bummed next time see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>end beautiful day venice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>watching becoming janelove moviethen shower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>real bad good internet day blocked woolies car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>floorset kicked butt tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>isofatv asked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>getting ready plumber gastronomically rewarded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                               text\n",
       "0           0  another case monday blues exhausted thinking m...\n",
       "1           1       rain nkotb 5 sexy wet grown men wish concert\n",
       "2           1  back gr8 walk cloudless blue heaven lifes wond...\n",
       "3           1  name common filler used simulate coconut milk ...\n",
       "4           0                                                 12\n",
       "5           1  im ya love rice green beans cheese fries crazi...\n",
       "6           0                                    0robertpatt now\n",
       "7           0                         watching tele went dentist\n",
       "8           1                             lolls thats pretty ace\n",
       "9           1  sinners would loved girly night in plan sumthi...\n",
       "10          0    terrible headache need poison brain immediately\n",
       "11          1                               love penny sleeptime\n",
       "12          0    le sigh righti sure pretty bummed next time see\n",
       "13          0                           end beautiful day venice\n",
       "14          1        watching becoming janelove moviethen shower\n",
       "15          1                                              think\n",
       "16          0  real bad good internet day blocked woolies car...\n",
       "17          0                       floorset kicked butt tonight\n",
       "18          1                                      isofatv asked\n",
       "19          1     getting ready plumber gastronomically rewarded"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorize(x):\n",
    "    x = x.lower()\n",
    "    document = nlp(x)\n",
    "    VecSum = np.zeros(shape = (1,K))\n",
    "    count = 0\n",
    "    for word in document:\n",
    "        if word.has_vector:\n",
    "            VecSum += word.vector.T\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return VecSum\n",
    "    return (VecSum / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300)\n",
      "[[ 2.75647527e-02  2.65810617e-01 -1.20142538e-01 ...  7.30983850e-02\n",
      "  -1.34846074e-01  2.14938232e-02]\n",
      " [-6.86142244e-02  1.23579780e-01 -6.00801477e-03 ... -4.20674475e-02\n",
      "   9.10800017e-02 -1.07077440e-01]\n",
      " [ 2.08118141e-01  1.75659141e-01 -8.58965028e-02 ...  1.16581925e-01\n",
      "  -4.68709982e-02  6.23359997e-02]\n",
      " ...\n",
      " [ 1.29627411e-04  2.58806168e-01 -1.29274338e-01 ... -7.29843341e-02\n",
      "  -3.50386677e-02  1.39453499e-01]\n",
      " [ 2.32800022e-02  4.42194998e-01  4.57715012e-01 ... -2.16530003e-02\n",
      "  -2.07570001e-01  6.07280009e-02]\n",
      " [-6.45246680e-02 -3.39993323e-01 -4.13046663e-01 ... -1.70766662e-01\n",
      "   1.10639667e-01  1.86332002e-01]]\n"
     ]
    }
   ],
   "source": [
    "DataMatrix = np.zeros(shape = (1,300))\n",
    "print(DataMatrix.shape)\n",
    "for text in Data.text:\n",
    "    DataMatrix = np.append(DataMatrix, Vectorize(text), axis = 0)\n",
    "DataMatrix = DataMatrix[1:]\n",
    "print(DataMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store labels in numpy array\n",
    "Labels = Data['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DataTrain, DataTest, LabelTrain, LabelTest = train_test_split(DataMatrix, Labels, test_size = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_Model = SVC(kernel = 'linear')\n",
    "SVM_Model.fit(DataTrain, LabelTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7193333333333334\n"
     ]
    }
   ],
   "source": [
    "print(SVM_Model.score(DataTest,LabelTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Dataa\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "H1 = 100\n",
    "H2 = 100\n",
    "OP = 1\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(K, H1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.prelu = nn.PReLU(1)\n",
    "        self.out = nn.Linear(H2, OP)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        a1 = self.fc1(input_)\n",
    "        h1 = self.relu1(a1)\n",
    "        dout = self.dout(h1)\n",
    "        a2 = self.fc2(dout)\n",
    "        h2 = self.prelu(a2)\n",
    "        a4 = self.out(h2)\n",
    "        y = self.out_act(a4)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attack(X,Y):\n",
    "    Net = NeuralNet()\n",
    "    Loss_Func = nn.BCELoss()\n",
    "    Optimizer = torch.optim.Adam(Net.parameters(), lr = LR, betas = (0.9, 0.999))\n",
    "    Torch_Dataset = Dataa.TensorDataset(X,Y)\n",
    "    Loader = Dataa.DataLoader(\n",
    "        dataset = Torch_Dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = 4,\n",
    "    )\n",
    "    for Epoch in range(EPOCH):\n",
    "        cnt = 0\n",
    "        s = 0\n",
    "        for step, (Batch_X,Batch_Y) in enumerate(Loader):\n",
    "            Optimizer.zero_grad()\n",
    "            B_X = Variable(Batch_X)\n",
    "            B_Y = Variable(Batch_Y)\n",
    "            Prediction = Net(B_X)\n",
    "            Loss = Loss_Func(Prediction.squeeze(),B_Y)\n",
    "            s = s + Loss.data.item()\n",
    "            cnt = cnt + 1\n",
    "            Loss.backward()\n",
    "            Optimizer.step()\n",
    "        print(s / cnt)\n",
    "    return Net\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5771439375611843\n",
      "0.5445202512763688\n",
      "0.5377944417615638\n",
      "0.52883053623952\n",
      "0.5150119730081604\n",
      "0.502000504124786\n",
      "0.49285336823966264\n",
      "0.4811704672202115\n",
      "0.47142542556140093\n",
      "0.4538855300201059\n",
      "0.4544689401419242\n",
      "0.44755798797189345\n",
      "0.43437821266210475\n",
      "0.42465571663673457\n",
      "0.4133352088207882\n",
      "0.4005848298211233\n",
      "0.40396378397659105\n",
      "0.3931547707714741\n",
      "0.39392606296104277\n",
      "0.3837767635688398\n",
      "0.3755260880030162\n",
      "0.36621894020039886\n",
      "0.36242619571739465\n",
      "0.3532877889642783\n",
      "0.35858480192685577\n",
      "0.3514748357624804\n",
      "0.34043269579726937\n",
      "0.3477782951005827\n",
      "0.3480149954557419\n",
      "0.33174603646047307\n",
      "0.3340433440502221\n",
      "0.3254505306023275\n",
      "0.3191419964329609\n",
      "0.31766173088155086\n",
      "0.31460421103366176\n",
      "0.3242412507534027\n",
      "0.31583655562022284\n",
      "0.3038435759137592\n",
      "0.30057318981789866\n",
      "0.30093487382146983\n",
      "0.2988256442744585\n",
      "0.2901618489333521\n",
      "0.30224935502096373\n",
      "0.29718956263003193\n",
      "0.2842427042105469\n",
      "0.2812114057982985\n",
      "0.28180506389358595\n",
      "0.2826674779726996\n",
      "0.2944898374060884\n",
      "0.2773452321535321\n",
      "0.29675351450511067\n",
      "0.2902658459205198\n",
      "0.2833197218546935\n",
      "0.2683238752927825\n",
      "0.26308674564788126\n",
      "0.2735217926909008\n",
      "0.2719572908399512\n",
      "0.27164421279989714\n",
      "0.28191080214499864\n",
      "0.2667184723585279\n",
      "0.250948459960485\n",
      "0.2697530904899559\n",
      "0.2531978404281829\n",
      "0.25634855609298884\n",
      "0.26200474449102346\n",
      "0.26805219545042347\n",
      "0.25435544636047563\n",
      "0.25563473712599954\n",
      "0.2546372409751065\n",
      "0.25075041374712476\n",
      "0.2536774194397703\n",
      "0.2566549720957663\n",
      "0.250586298307607\n",
      "0.24898368167863072\n",
      "0.24152864448690867\n",
      "0.24328494155823618\n",
      "0.25391597787174286\n",
      "0.2534038241670171\n",
      "0.24855108751575528\n",
      "0.2545334853108319\n",
      "0.2568143688813205\n",
      "0.2409196385398719\n",
      "0.23591155030927952\n",
      "0.22831626210838415\n",
      "0.2369664330300279\n",
      "0.23539309080036896\n",
      "0.24449276999168768\n",
      "0.231911062302677\n",
      "0.23278263076186462\n",
      "0.22855913263891264\n",
      "0.23515212444050052\n",
      "0.24178041124019012\n",
      "0.23725557449975568\n",
      "0.2297841692740601\n",
      "0.2285482750331621\n",
      "0.2313675374957905\n",
      "0.23149395296217706\n",
      "0.2293728123114431\n",
      "0.2242798953189104\n",
      "0.22492505433435123\n"
     ]
    }
   ],
   "source": [
    "DataTensor = torch.from_numpy(DataTrain).float()\n",
    "LabelTensor = torch.from_numpy(LabelTrain).float()\n",
    "Network = Attack(DataTensor,LabelTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTestTensor = torch.from_numpy(DataTest).float()\n",
    "LabelTestTensor = torch.from_numpy(LabelTest).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid = Network(DataTestTensor).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = Valid.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mask = [(1 if val > 0.5 else 0) for val in Output]\n",
    "Accuracy = sum(Mask == LabelTest) / len(Mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6993333333333334\n"
     ]
    }
   ],
   "source": [
    "print(Accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
